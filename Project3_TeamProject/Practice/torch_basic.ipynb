{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.5.1+cu124'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 기본적인 텐서 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.1289e-36,  1.0243e-42],\n",
       "        [ 0.0000e+00,  0.0000e+00],\n",
       "        [ 0.0000e+00,  0.0000e+00],\n",
       "        [ 0.0000e+00,  0.0000e+00]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.empty(4,2)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3831, 0.7022],\n",
       "        [0.7381, 0.0017],\n",
       "        [0.2649, 0.6227],\n",
       "        [0.3138, 0.8010]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.rand(4,2)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0],\n",
       "        [0, 0],\n",
       "        [0, 0],\n",
       "        [0, 0]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.zeros(4,2,dtype=torch.long)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3.0000, 2.3000])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([3, 2.3])\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.]], dtype=torch.float64)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = x.new_ones(2, 4, dtype=torch.double)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.2240, -0.2892,  1.4559, -2.1971],\n",
       "        [ 1.0893,  0.8927,  0.1365, -0.3622]], dtype=torch.float64)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn_like(x, dtype=torch.double)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 4])\n"
     ]
    }
   ],
   "source": [
    "print(x.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 2., 3.])\n",
      "torch.float32\n"
     ]
    }
   ],
   "source": [
    "ft = torch.FloatTensor([1,2,3])\n",
    "print(ft)\n",
    "print(ft.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CUDA Tensors\n",
    "* .to 메소드로 텐서를 cpu, gpu로 옮길 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.6229])\n",
      "0.6228950619697571\n",
      "torch.float32\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(1)\n",
    "print(x)\n",
    "print(x.item())\n",
    "print(x.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "tensor([1.], device='cuda:0')\n",
      "tensor([0.6229], device='cuda:0')\n",
      "tensor([1.6229], device='cuda:0')\n",
      "tensor([1.6229], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "y = torch.ones_like(x, device=device)\n",
    "print(y)\n",
    "\n",
    "x = x.to(device)\n",
    "print(x)\n",
    "\n",
    "z = x+y\n",
    "print(z)\n",
    "\n",
    "print(z.to('cpu', torch.double))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 연산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.4450, -0.3918]])\n",
      "tensor([[0.4450, 0.3918]])\n",
      "tensor([[1., -0.]])\n",
      "tensor([[ 0., -1.]])\n",
      "tensor([[ 0.1000, -0.1000]])\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "a = torch.rand(1,2)*2-1\n",
    "print(a)\n",
    "print(torch.abs(a))\n",
    "print(torch.ceil(a))\n",
    "print(torch.floor(a))\n",
    "print(torch.clamp(a, -0.1, 0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9094, 0.8896],\n",
      "        [0.0798, 0.7641]])\n",
      "torch.return_types.max(\n",
      "values=tensor([0.9094, 0.8896]),\n",
      "indices=tensor([0, 0]))\n",
      "torch.return_types.max(\n",
      "values=tensor([0.9094, 0.7641]),\n",
      "indices=tensor([0, 1]))\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(2,2)\n",
    "print(x)\n",
    "print(x.max(dim=0))\n",
    "print(x.max(dim=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 인덱싱"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 2.],\n",
      "        [3., 4.]])\n",
      "tensor(1.)\n",
      "tensor(2.)\n",
      "tensor(3.)\n",
      "tensor(4.)\n",
      "tensor([1., 3.])\n",
      "tensor([2., 4.])\n",
      "tensor([1., 2.])\n",
      "tensor([3., 4.])\n"
     ]
    }
   ],
   "source": [
    "x = torch.Tensor([[1,2], [3,4]])\n",
    "\n",
    "print(x)\n",
    "\n",
    "print(x[0,0])\n",
    "print(x[0,1])\n",
    "print(x[1,0])\n",
    "print(x[1,1])\n",
    "\n",
    "print(x[:,0])\n",
    "print(x[:,1])\n",
    "\n",
    "print(x[0,:])\n",
    "print(x[1,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 크기나 모양 변경"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.4053,  1.1408, -1.2392,  2.0316,  0.8105],\n",
      "        [ 0.5459,  1.8204, -0.7542,  1.2955, -0.0757],\n",
      "        [-0.0413,  0.5955, -0.8474, -1.4523,  2.3011],\n",
      "        [-0.5990, -0.5245,  1.7594, -0.8098, -0.4066]])\n",
      "tensor([-0.4053,  1.1408, -1.2392,  2.0316,  0.8105,  0.5459,  1.8204, -0.7542,\n",
      "         1.2955, -0.0757, -0.0413,  0.5955, -0.8474, -1.4523,  2.3011, -0.5990,\n",
      "        -0.5245,  1.7594, -0.8098, -0.4066])\n",
      "tensor([[-0.4053,  1.1408, -1.2392,  2.0316],\n",
      "        [ 0.8105,  0.5459,  1.8204, -0.7542],\n",
      "        [ 1.2955, -0.0757, -0.0413,  0.5955],\n",
      "        [-0.8474, -1.4523,  2.3011, -0.5990],\n",
      "        [-0.5245,  1.7594, -0.8098, -0.4066]])\n",
      "tensor([[-0.4053,  1.1408],\n",
      "        [-1.2392,  2.0316],\n",
      "        [ 0.8105,  0.5459],\n",
      "        [ 1.8204, -0.7542],\n",
      "        [ 1.2955, -0.0757],\n",
      "        [-0.0413,  0.5955],\n",
      "        [-0.8474, -1.4523],\n",
      "        [ 2.3011, -0.5990],\n",
      "        [-0.5245,  1.7594],\n",
      "        [-0.8098, -0.4066]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(4,5)\n",
    "print(x)\n",
    "y = x.view(20)\n",
    "print(y)\n",
    "z = x.view(5,-1)\n",
    "print(z)\n",
    "m = x.view(-1, 2)\n",
    "print(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### item\n",
    "스칼라가 하나만 있어야 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-2.2040650844573975\n",
      "torch.float32\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(1)\n",
    "print(x.item())\n",
    "print(x.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### squeeze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.1352, 0.0085, 0.2651],\n",
      "         [0.0782, 0.2405, 0.2358],\n",
      "         [0.2804, 0.3154, 0.4986]]])\n",
      "torch.Size([1, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "tensor = torch.rand(1,3,3)\n",
    "print(tensor)\n",
    "print(tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1352, 0.0085, 0.2651],\n",
      "        [0.0782, 0.2405, 0.2358],\n",
      "        [0.2804, 0.3154, 0.4986]])\n",
      "torch.Size([3, 3])\n"
     ]
    }
   ],
   "source": [
    "t = tensor.squeeze()\n",
    "print(t)\n",
    "print(t.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.1352, 0.0085, 0.2651],\n",
       "         [0.0782, 0.2405, 0.2358],\n",
       "         [0.2804, 0.3154, 0.4986]]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.unsqueeze(dim=0) # 1,3,3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.1352, 0.0085, 0.2651]],\n",
       "\n",
       "        [[0.0782, 0.2405, 0.2358]],\n",
       "\n",
       "        [[0.2804, 0.3154, 0.4986]]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.unsqueeze(dim=1) # 3,1,3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 4.])\n",
      "tensor([2., 5.])\n",
      "tensor([3., 6.])\n",
      "tensor([[1., 4.],\n",
      "        [2., 5.],\n",
      "        [3., 6.]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.FloatTensor([1,4])\n",
    "print(x)\n",
    "y = torch.FloatTensor([2,5])\n",
    "print(y)\n",
    "z = torch.FloatTensor([3,6])\n",
    "print(z)\n",
    "\n",
    "print(torch.stack([x,y,z]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 1.2180, -0.8358, -0.6941],\n",
      "         [ 0.2835, -1.8649, -0.8072],\n",
      "         [ 1.3806, -0.5297, -0.1719]]])\n",
      "tensor([[[-1.8086,  0.2357,  0.1399],\n",
      "         [ 0.2157, -0.0529,  0.1281],\n",
      "         [ 1.5482, -1.1254, -1.1607]]])\n",
      "tensor([[[ 1.2180, -0.8358, -0.6941],\n",
      "         [ 0.2835, -1.8649, -0.8072],\n",
      "         [ 1.3806, -0.5297, -0.1719]],\n",
      "\n",
      "        [[-1.8086,  0.2357,  0.1399],\n",
      "         [ 0.2157, -0.0529,  0.1281],\n",
      "         [ 1.5482, -1.1254, -1.1607]]])\n",
      "torch.Size([2, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "a = torch.randn(1,3,3)\n",
    "print(a)\n",
    "b = torch.randn(1,3,3)\n",
    "print(b)\n",
    "c = torch.cat((a,b), dim=0)\n",
    "print(c)\n",
    "print(c.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 1.2180, -0.8358, -0.6941],\n",
      "         [ 0.2835, -1.8649, -0.8072],\n",
      "         [ 1.3806, -0.5297, -0.1719],\n",
      "         [-1.8086,  0.2357,  0.1399],\n",
      "         [ 0.2157, -0.0529,  0.1281],\n",
      "         [ 1.5482, -1.1254, -1.1607]]])\n",
      "torch.Size([1, 6, 3])\n"
     ]
    }
   ],
   "source": [
    "c = torch.cat((a,b), dim=1)\n",
    "print(c)\n",
    "print(c.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 1.2180, -0.8358, -0.6941, -1.8086,  0.2357,  0.1399],\n",
      "         [ 0.2835, -1.8649, -0.8072,  0.2157, -0.0529,  0.1281],\n",
      "         [ 1.3806, -0.5297, -0.1719,  1.5482, -1.1254, -1.1607]]])\n",
      "torch.Size([1, 3, 6])\n"
     ]
    }
   ],
   "source": [
    "c = torch.cat((a,b), dim=2)\n",
    "print(c)\n",
    "print(c.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.8262, 0.7280, 0.5111, 0.3008, 0.6205, 0.4830],\n",
      "        [0.0637, 0.0255, 0.1915, 0.6094, 0.1251, 0.6518],\n",
      "        [0.8132, 0.7773, 0.7062, 0.4638, 0.0351, 0.9031]])\n"
     ]
    }
   ],
   "source": [
    "tensor = torch.rand(3,6)\n",
    "print(tensor)\n",
    "\n",
    "t1, t2, t3 = torch.chunk(tensor, 3, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.8262, 0.7280],\n",
      "        [0.0637, 0.0255],\n",
      "        [0.8132, 0.7773]])\n",
      "tensor([[0.5111, 0.3008],\n",
      "        [0.1915, 0.6094],\n",
      "        [0.7062, 0.4638]])\n",
      "tensor([[0.6205, 0.4830],\n",
      "        [0.1251, 0.6518],\n",
      "        [0.0351, 0.9031]])\n"
     ]
    }
   ],
   "source": [
    "print(t1)\n",
    "print(t2)\n",
    "print(t3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.8262, 0.7280, 0.5111, 0.3008, 0.6205, 0.4830]])\n",
      "tensor([[0.0637, 0.0255, 0.1915, 0.6094, 0.1251, 0.6518]])\n",
      "tensor([[0.8132, 0.7773, 0.7062, 0.4638, 0.0351, 0.9031]])\n"
     ]
    }
   ],
   "source": [
    "t1, t2, t3 = torch.chunk(tensor, 3, dim=0)\n",
    "\n",
    "print(t1)\n",
    "print(t2)\n",
    "print(t3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### split\n",
    "chunk와 달리 어떤 크기의 텐서로 나눌 것인지를 입력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor = torch.rand(3,6)\n",
    "t1, t2 = torch.split(tensor, 3, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3759, 0.1123, 0.1188],\n",
      "        [0.8110, 0.5877, 0.7239],\n",
      "        [0.5224, 0.8058, 0.7932]])\n",
      "tensor([[0.2966, 0.0704, 0.9647],\n",
      "        [0.5114, 0.5513, 0.9471],\n",
      "        [0.0285, 0.5061, 0.7753]])\n"
     ]
    }
   ],
   "source": [
    "print(t1)\n",
    "print(t2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### torch와 numpy의 상호변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 1., 1., 1., 1.])\n",
      "[1. 1. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "a = torch.ones(6)\n",
    "print(a)\n",
    "b = a.numpy()\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2., 2., 2., 2., 2., 2.])\n",
      "[2. 2. 2. 2. 2. 2.]\n"
     ]
    }
   ],
   "source": [
    "a.add_(1)\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "텐서가 cpu상에 있으면 메모리를 공유하므로 같이 변함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2. 2. 2. 2. 2. 2. 2.]\n",
      "tensor([2., 2., 2., 2., 2., 2., 2.], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "a = np.ones(7)\n",
    "b = torch.from_numpy(a)\n",
    "np.add(a, 1, out=a)\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autograd(자동미분)\n",
    "\n",
    "* torch.autograd: Tensor의 모든 연산에 대해 자동 미분 제공\n",
    "* 역전파(backprop)에 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 2.0203,  1.5061,  6.8237],\n",
      "        [-2.4898, -2.2844,  1.4609],\n",
      "        [-2.8778, -2.7915,  9.2767]])\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "a = torch.randn(3,3)\n",
    "a*=3\n",
    "print(a)\n",
    "print(a.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "tensor(168.5964, grad_fn=<SumBackward0>)\n",
      "<SumBackward0 object at 0x0000018EC7F7EB00>\n"
     ]
    }
   ],
   "source": [
    "a.requires_grad_(True)\n",
    "print(a.requires_grad)\n",
    "\n",
    "b = (a*a).sum()\n",
    "print(b)\n",
    "print(b.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 그래디언트 구하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones(3, 3, requires_grad=True)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[6., 6., 6.],\n",
      "        [6., 6., 6.],\n",
      "        [6., 6., 6.]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "y = x+5\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[36., 36., 36.],\n",
      "        [36., 36., 36.],\n",
      "        [36., 36., 36.]], grad_fn=<MulBackward0>) tensor(36., grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "z = y*y\n",
    "out = z.mean()\n",
    "print(z, out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(36., grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(out)\n",
    "out.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]], requires_grad=True)\n",
      "tensor([[1.3333, 1.3333, 1.3333],\n",
      "        [1.3333, 1.3333, 1.3333],\n",
      "        [1.3333, 1.3333, 1.3333]])\n"
     ]
    }
   ],
   "source": [
    "print(x)\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ -125.0663, -1321.9078,  1282.9073], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(3, requires_grad=True)\n",
    "\n",
    "y = x*2\n",
    "while y.data.norm() < 1000:\n",
    "    y = y*2\n",
    "\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.0240e+02, 1.0240e+03, 1.0240e-01])\n"
     ]
    }
   ],
   "source": [
    "v = torch.tensor([0.1, 1.0, 0.0001], dtype=torch.float)\n",
    "y.backward(v)\n",
    "\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### with torch.no_grad()\n",
    "* 기울기 계산을 하지 않지만 매개변수를 갖는 모델을 평가할 때 유용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "print(x.requires_grad)\n",
    "print((x**2).requires_grad)\n",
    "\n",
    "with torch.no_grad():\n",
    "    print(x.requires_grad)\n",
    "    print((x**2).requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### detach\n",
    "내용물은 같으나, require_grad가 다른 새로운 Tensor를 가져올 때"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n",
      "tensor(True)\n"
     ]
    }
   ],
   "source": [
    "print(x.requires_grad) # T\n",
    "y = x.detach()\n",
    "print(y.requires_grad) # F\n",
    "print(x.eq(y).all())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backpropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1.],\n",
      "        [1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.ones(2,2)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1.],\n",
      "        [1., 1.]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "a = torch.ones(2,2,requires_grad=True)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1.],\n",
      "        [1., 1.]])\n",
      "None\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(a.data)\n",
    "print(a.grad)\n",
    "print(a.grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3., 3.],\n",
      "        [3., 3.]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "b = a + 2\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[9., 9.],\n",
      "        [9., 9.]], grad_fn=<PowBackward0>)\n"
     ]
    }
   ],
   "source": [
    "c = b ** 2\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(36., grad_fn=<SumBackward0>)\n"
     ]
    }
   ],
   "source": [
    "out = c.sum()\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "out.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[6., 6.],\n",
      "        [6., 6.]])\n"
     ]
    }
   ],
   "source": [
    "print(a.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "<PowBackward0 object at 0x0000018E9604FB50>\n",
      "tensor([[9., 9.],\n",
      "        [9., 9.]])\n",
      "\n",
      "\n",
      "None\n",
      "<AddBackward0 object at 0x0000018EC7F7ECE0>\n",
      "tensor([[3., 3.],\n",
      "        [3., 3.]])\n",
      "\n",
      "\n",
      "tensor([[6., 6.],\n",
      "        [6., 6.]])\n",
      "None\n",
      "tensor([[1., 1.],\n",
      "        [1., 1.]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jongcheol\\AppData\\Local\\Temp\\ipykernel_15652\\1042403374.py:1: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\build\\aten\\src\\ATen/core/TensorBody.h:494.)\n",
      "  print(c.grad)\n",
      "C:\\Users\\jongcheol\\AppData\\Local\\Temp\\ipykernel_15652\\1042403374.py:5: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\build\\aten\\src\\ATen/core/TensorBody.h:494.)\n",
      "  print(b.grad)\n"
     ]
    }
   ],
   "source": [
    "print(c.grad)\n",
    "print(c.grad_fn)\n",
    "print(c.data)\n",
    "print('\\n')\n",
    "print(b.grad)\n",
    "print(b.grad_fn)\n",
    "print(b.data)\n",
    "print('\\n')\n",
    "print(a.grad)\n",
    "print(a.grad_fn)\n",
    "print(a.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "<SumBackward0 object at 0x0000018E960AF070>\n",
      "tensor(36.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jongcheol\\AppData\\Local\\Temp\\ipykernel_15652\\1499947373.py:1: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\build\\aten\\src\\ATen/core/TensorBody.h:494.)\n",
      "  print(out.grad)\n"
     ]
    }
   ],
   "source": [
    "print(out.grad)\n",
    "print(out.grad_fn)\n",
    "print(out.data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
