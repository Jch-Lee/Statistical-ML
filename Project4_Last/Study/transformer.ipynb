{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines= pd.read_table('english to french/english to french.txt', names=['eng', 'fr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    eng        fr\n",
      "0   Go.      Va !\n",
      "1  Run!   Cours !\n",
      "2  Run!  Courez !\n",
      "                            eng                              fr\n",
      "49997  They go to work on foot.     Ils vont au travail à pied.\n",
      "49998  They got into the train.    Ils montèrent dans le train.\n",
      "49999  They got into the train.  Elles montèrent dans le train.\n"
     ]
    }
   ],
   "source": [
    "lines = lines[0:50000]\n",
    "print(lines.head(3))\n",
    "print(lines.tail(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines['eng']=lines['eng'].apply(lambda x: x.lower())\n",
    "lines['fr']=lines['fr'].apply(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "exclude = set(string.punctuation)\n",
    "lines.eng=lines.eng.apply(lambda x: ''.join(ch for ch in x if ch not in exclude))\n",
    "lines.fr=lines.fr.apply(lambda x: ''.join(ch for ch in x if ch not in exclude))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   eng       fr\n",
      "0   go      va \n",
      "1  run   cours \n",
      "2  run  courez \n",
      "                           eng                             fr\n",
      "49997  they go to work on foot     ils vont au travail à pied\n",
      "49998  they got into the train    ils montèrent dans le train\n",
      "49999  they got into the train  elles montèrent dans le train\n"
     ]
    }
   ],
   "source": [
    "print(lines.head(3))\n",
    "print(lines.tail(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines.fr = lines.fr.apply(lambda x : 'start '+ x + ' end')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   eng                 fr\n",
      "0   go      start va  end\n",
      "1  run   start cours  end\n",
      "2  run  start courez  end\n",
      "                           eng                                       fr\n",
      "49997  they go to work on foot     start ils vont au travail à pied end\n",
      "49998  they got into the train    start ils montèrent dans le train end\n",
      "49999  they got into the train  start elles montèrent dans le train end\n"
     ]
    }
   ],
   "source": [
    "print(lines.head(3))\n",
    "print(lines.tail(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 2)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit a tokenizer\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "def create_tokenizer(lines):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(lines)\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "eng_tokenizer = create_tokenizer(lines['eng'])\n",
    "eng_dict=json.loads(json.dumps(eng_tokenizer.word_counts))\n",
    "df =pd.DataFrame([eng_dict.keys(), eng_dict.values()]).T\n",
    "df.columns = ['word','count']\n",
    "\n",
    "df = df.sort_values(by='count',ascending = False)\n",
    "df['cum_count']=df['count'].cumsum()\n",
    "df['cum_perc'] = df['cum_count']/df['cum_count'].max()\n",
    "final_eng_words = df[df['cum_perc']<0.8]['word'].values\n",
    "#final_eng_words=df['word'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "fr_tokenizer = create_tokenizer(lines['fr'])\n",
    "fr_dict = json.loads(json.dumps(fr_tokenizer.word_counts))\n",
    "df =pd.DataFrame([fr_dict.keys(), fr_dict.values()]).T\n",
    "df.columns = ['word','count']\n",
    "df = df.sort_values(by='count',ascending = False)\n",
    "df['cum_count']=df['count'].cumsum()\n",
    "df['cum_perc'] = df['cum_count']/df['cum_count'].max()\n",
    "final_fr_words = df[df['cum_perc']<0.8]['word'].values\n",
    "#final_fr_words=df['word'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "384 357\n"
     ]
    }
   ],
   "source": [
    "print(len(final_eng_words),len(final_fr_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_eng_words(x):\n",
    "  t = []\n",
    "  x = x.split()\n",
    "  for i in range(len(x)):\n",
    "    if x[i] in final_eng_words:\n",
    "      t.append(x[i])\n",
    "    else:\n",
    "      t.append('unk')\n",
    "  x3 = ''\n",
    "  for i in range(len(t)):\n",
    "    x3 = x3+t[i]+' '\n",
    "  return x3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'he is unk good '"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filter_eng_words('he is extremely good')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_fr_words(x):\n",
    "  t = []\n",
    "  x = x.split()\n",
    "  for i in range(len(x)):\n",
    "    if x[i] in final_fr_words:\n",
    "      t.append(x[i])\n",
    "    else:\n",
    "      t.append('unk')\n",
    "  x3 = ''\n",
    "  for i in range(len(t)):\n",
    "    x3 = x3+t[i]+' '\n",
    "  return x3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines['eng']=lines['eng'].apply(filter_eng_words)\n",
    "lines['fr']=lines['fr'].apply(filter_fr_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_eng_words=set()\n",
    "for eng in lines.eng:\n",
    "    for word in eng.split():\n",
    "        if word not in all_eng_words:\n",
    "            all_eng_words.add(word)\n",
    "    \n",
    "all_french_words=set()\n",
    "for fr in lines.fr:\n",
    "    for word in fr.split():\n",
    "        if word not in all_french_words:\n",
    "            all_french_words.add(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_words = sorted(list(all_eng_words))\n",
    "target_words = sorted(list(all_french_words))\n",
    "num_encoder_tokens = len(all_eng_words)\n",
    "num_decoder_tokens = len(all_french_words)\n",
    "# del all_eng_words, all_french_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'unk'}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(all_french_words) - set(final_fr_words) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "385"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_eng_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "358"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(target_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_token_index = dict(\n",
    "    [(word, i+1) for i, word in enumerate(input_words)])\n",
    "target_token_index = dict(\n",
    "    [(word, i+1) for i, word in enumerate(target_words)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "358"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_decoder_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "284\n",
      "89\n",
      "youve\n",
      "start\n",
      "end\n"
     ]
    }
   ],
   "source": [
    "print(target_token_index['start']) \n",
    "print(target_token_index['end']) \n",
    "print(list(input_token_index.keys())[384])\n",
    "print(list(target_token_index.keys())[283])\n",
    "print(list(target_token_index.keys())[88])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "length_list=[]\n",
    "for l in lines.fr:\n",
    "    length_list.append(len(l.split(' ')))\n",
    "fr_max_length = np.max(length_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "length_list=[]\n",
    "for l in lines.eng:\n",
    "    length_list.append(len(l.split(' ')))\n",
    "eng_max_length = np.max(length_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "17\n"
     ]
    }
   ],
   "source": [
    "print(eng_max_length)\n",
    "print(fr_max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input_data = np.zeros(\n",
    "    (len(lines['eng']), eng_max_length),\n",
    "    dtype='float32')\n",
    "decoder= np.zeros(\n",
    "    (len(lines['fr']), fr_max_length+1),\n",
    "    dtype='float32')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 18)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (input_text, target_text) in enumerate(zip(lines['eng'], lines['fr'])):\n",
    "    for t, word in enumerate(input_text.split()):\n",
    "        encoder_input_data[i, t] = input_token_index[word]\n",
    "    for t, word in enumerate(target_text.split()):\n",
    "        decoder[i, t] = target_token_index[word]\n",
    "decoder_input_data=decoder[:,:-1]\n",
    "decoder_target_data=decoder[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 8) (50000, 17) (50000, 17)\n"
     ]
    }
   ],
   "source": [
    "print(encoder_input_data.shape,decoder_input_data.shape,decoder_target_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[63.  8.  0.  0.  0.  0.  0.  0.]\n",
      "[284. 320. 274.  89.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.]\n",
      "[320. 274.  89.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.]\n"
     ]
    }
   ],
   "source": [
    "print(encoder_input_data[1000])\n",
    "print(decoder_input_data[1000])\n",
    "print(decoder_target_data[1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "def get_angles(pos, i, d_model):\n",
    "  angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))\n",
    "  return pos * angle_rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_encoding(position, d_model):\n",
    "  angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n",
    "                          np.arange(d_model)[np.newaxis, :],\n",
    "                          d_model)\n",
    "\n",
    "  # apply sin to even indices in the array; 2i\n",
    "  angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "\n",
    "  # apply cos to odd indices in the array; 2i+1\n",
    "  angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "\n",
    "  pos_encoding = angle_rads[np.newaxis, ...]\n",
    "\n",
    "  return tf.cast(pos_encoding, dtype=tf.float32)#(1,position,d_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_padding_mask(x):\n",
    "  mask = tf.cast(tf.math.equal(x, 0), tf.float32)\n",
    "  #x:tokenized words with padding for each sentence 1D텐서 [...,0,0,0]\n",
    "  # add extra dimensions to add the padding\n",
    "  # to the attention logits.\n",
    "  return mask[:, tf.newaxis, tf.newaxis, :]  # (batch, 1, 1, seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_look_ahead_mask(x):\n",
    "    seq_len=tf.shape(x)[1]\n",
    "    look_ahead_mask=1-tf.linalg.band_part(tf.ones((seq_len,seq_len)),-1,0)#lower,upper\n",
    "    padding_mask=create_padding_mask(x)\n",
    "    return tf.maximum(look_ahead_mask,padding_mask)\n",
    "\n",
    "##x:tokenized words with padding for each sentence 1D텐서 [...,0,0,0](ts,), return (ts,ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(q, k, v, mask):\n",
    " \n",
    "\n",
    "  matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n",
    "\n",
    "  # scale matmul_qk\n",
    "  dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
    "  scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
    "\n",
    "  if mask is not None:\n",
    "    scaled_attention_logits += (mask * -1e9)\n",
    "\n",
    "  # softmax is normalized on the last axis (seq_len_k) so that the scores\n",
    "  # add up to 1.\n",
    "  attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k) (number of heads,ts,ts)\n",
    "\n",
    "  output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v) (number of heads,ts,64)\n",
    "\n",
    "  return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "  def __init__(self, d_model, num_heads):\n",
    "    super(MultiHeadAttention, self).__init__()\n",
    "    self.num_heads = num_heads  #8\n",
    "    self.d_model = d_model #128\n",
    "\n",
    "    assert d_model % self.num_heads == 0\n",
    "\n",
    "    self.depth = d_model // self.num_heads\n",
    "\n",
    "    self.wq = tf.keras.layers.Dense(d_model) \n",
    "    self.wk = tf.keras.layers.Dense(d_model) \n",
    "    self.wv = tf.keras.layers.Dense(d_model) \n",
    "\n",
    "    self.dense = tf.keras.layers.Dense(d_model) # scale_dot_attention후 concatenate한 attention (ts,128)를 Dense에 통과하기 위함\n",
    "\n",
    "  def split_heads(self, x, batch_size):\n",
    "   \n",
    "    x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
    "    return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "\n",
    "  def call(self, v, k, q, mask):\n",
    "    batch_size = tf.shape(q)[0]\n",
    "\n",
    "    q = self.wq(q)  \n",
    "    k = self.wk(k) \n",
    "    v = self.wv(v) \n",
    "\n",
    "    q = self.split_heads(q, batch_size) \n",
    "    k = self.split_heads(k, batch_size)  \n",
    "    v = self.split_heads(v, batch_size) \n",
    "\n",
    "    \n",
    "    scaled_attention, attention_weights = scaled_dot_product_attention(\n",
    "        q, k, v, mask)\n",
    "\n",
    "    scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, depth)\n",
    "\n",
    "    concat_attention = tf.reshape(scaled_attention,\n",
    "                                  (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n",
    "\n",
    "    output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n",
    "\n",
    "    return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def point_wise_feed_forward_network(d_model, dff):\n",
    "  return tf.keras.Sequential([\n",
    "      tf.keras.layers.Dense(dff, activation='relu'),  # (batch_size, seq_len, dff) seq_len=ts\n",
    "      tf.keras.layers.Dense(d_model)  # (batch_size, seq_len, d_model) d_model=128\n",
    "  ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "  def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "    super(EncoderLayer, self).__init__()\n",
    "\n",
    "    self.mha = MultiHeadAttention(d_model, num_heads)\n",
    "    self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "\n",
    "    self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "    self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "    self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "    self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "  def call(self, x, mask):\n",
    "\n",
    "    attn_output, _ = self.mha(x, x, x, mask)  # (batch_size, input_seq_len, d_model) input_seq_len=input_ts, d_model=128\n",
    "    attn_output = self.dropout1(attn_output)\n",
    "    out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n",
    "\n",
    "    ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n",
    "    ffn_output = self.dropout2(ffn_output)\n",
    "    out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n",
    "\n",
    "    return out2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "  def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "    super(DecoderLayer, self).__init__()\n",
    "\n",
    "    self.mha1 = MultiHeadAttention(d_model, num_heads)\n",
    "    self.mha2 = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "    self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "\n",
    "    self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "    self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "    self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "    self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "    self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "    self.dropout3 = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "  def call(self, x, enc_output, look_ahead_mask, padding_mask):\n",
    "    # enc_output.shape == (batch_size, input_seq_len, d_model) x: decoder input\n",
    "\n",
    "    attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask)  # (batch_size, target_seq_len, d_model) \n",
    "    attn1 = self.dropout1(attn1)                   #targer_seq_len=decoder_input_ts\n",
    "    out1 = self.layernorm1(attn1 + x)\n",
    "\n",
    "    attn2, attn_weights_block2 = self.mha2(enc_output, enc_output, out1, padding_mask)\n",
    "    attn2 = self.dropout2(attn2)\n",
    "    out2 = self.layernorm2(attn2 + out1)  # (batch_size, target_seq_len, d_model) #target_seq_len=decoder_input_ts\n",
    "\n",
    "    ffn_output = self.ffn(out2)  # (batch_size, target_seq_len, d_model)\n",
    "    ffn_output = self.dropout3(ffn_output)\n",
    "    out3 = self.layernorm3(ffn_output + out2)  # (batch_size, target_seq_len, d_model)\n",
    "\n",
    "    return out3, attn_weights_block1, attn_weights_block2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.layers.Layer):\n",
    "  def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size,\n",
    "               maximum_position_encoding, rate=0.1):\n",
    "    super(Encoder, self).__init__()\n",
    "\n",
    "    self.d_model = d_model\n",
    "    self.num_layers = num_layers\n",
    "\n",
    "    self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)\n",
    "    self.pos_encoding = positional_encoding(maximum_position_encoding,\n",
    "                                            self.d_model)\n",
    "\n",
    "    self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate)\n",
    "                       for _ in range(num_layers)]\n",
    "\n",
    "    self.dropout = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "  def call(self, x, mask):\n",
    "\n",
    "    seq_len = tf.shape(x)[1]  #x는 tokenized sentence with padding\n",
    "\n",
    "    # adding embedding and position encoding.\n",
    "    x = self.embedding(x)  # (batch_size, input_seq_len, d_model)\n",
    "    x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "    x += self.pos_encoding[:, :seq_len, :]\n",
    "\n",
    "    x = self.dropout(x)\n",
    "\n",
    "    for i in range(self.num_layers):\n",
    "      x = self.enc_layers[i](x, mask)\n",
    "\n",
    "    return x  # (batch_size, input_seq_len, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'a_1': 0, 'b_1': 0, 'a_2': 1, 'b_2': 1}\n"
     ]
    }
   ],
   "source": [
    "a={}\n",
    "for i in range(2):\n",
    "    block1=i**2\n",
    "    block2=i**3\n",
    "    a[\"a_{}\".format(i+1)]=block1\n",
    "    a[\"b_{}\".format(i+1)]=block2\n",
    "print(a)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.layers.Layer):\n",
    "  def __init__(self, num_layers, d_model, num_heads, dff, target_vocab_size,\n",
    "               maximum_position_encoding, rate=0.1):\n",
    "    super(Decoder, self).__init__()\n",
    "\n",
    "    self.d_model = d_model\n",
    "    self.num_layers = num_layers\n",
    "\n",
    "    self.embedding = tf.keras.layers.Embedding(target_vocab_size, d_model)\n",
    "    self.pos_encoding = positional_encoding(maximum_position_encoding, d_model)\n",
    "\n",
    "    self.dec_layers = [DecoderLayer(d_model, num_heads, dff, rate)\n",
    "                       for _ in range(num_layers)]\n",
    "    self.dropout = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "  def call(self, x, enc_output, look_ahead_mask, padding_mask):\n",
    "\n",
    "    seq_len = tf.shape(x)[1]\n",
    "    attention_weights = {}\n",
    "\n",
    "    x = self.embedding(x)  # (batch_size, target_seq_len, d_model)\n",
    "    x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "    x += self.pos_encoding[:, :seq_len, :]\n",
    "\n",
    "    x = self.dropout(x)\n",
    "\n",
    "    for i in range(self.num_layers):\n",
    "      x, block1, block2 = self.dec_layers[i](x, enc_output,look_ahead_mask, padding_mask)\n",
    "\n",
    "      attention_weights[f'decoder_layer{i+1}_block1'] = block1\n",
    "      attention_weights[f'decoder_layer{i+1}_block2'] = block2\n",
    "\n",
    "    # x.shape == (batch_size, target_seq_len, d_model)\n",
    "    return x, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformer(num_layers, d_model, num_heads, dff, input_vocab_size,\n",
    "               target_vocab_size, ts_input, ts_target, rate=0.1):\n",
    "    \n",
    "    enc_input = tf.keras.Input(shape=(None,))  \n",
    "    dec_input = tf.keras.Input(shape=(None,))\n",
    "    \n",
    "    enc_padding_mask=tf.keras.layers.Lambda(create_padding_mask,output_shape=(1,1,None))(enc_input)\n",
    "    look_ahead_mask=tf.keras.layers.Lambda(create_look_ahead_mask,output_shape=(1,None,None))(dec_input)\n",
    "    dec_padding_mask=tf.keras.layers.Lambda(create_padding_mask,output_shape=(1,1,None))(enc_input)\n",
    "    \n",
    "    enc_output=Encoder(num_layers, d_model, num_heads, dff,\n",
    "                             input_vocab_size, ts_input, rate)(enc_input,enc_padding_mask)\n",
    "    dec_output,_ =Decoder(num_layers, d_model, num_heads, dff,\n",
    "                           target_vocab_size, ts_target, rate)(dec_input,enc_output,look_ahead_mask,dec_padding_mask)\n",
    "    outputs=tf.keras.layers.Dense(target_vocab_size)(dec_output)\n",
    "    \n",
    "    return tf.keras.Model(inputs=[enc_input, dec_input], outputs=outputs)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([False False  True], shape=(3,), dtype=bool)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "a=[False,False,True]\n",
    "b=[False,True,True]\n",
    "c=tf.math.logical_and(a,b)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(y_true, y_pred):\n",
    "  mask = tf.math.logical_not(tf.math.equal(y_true, 0))\n",
    "  loss_ = loss_object(y_true, y_pred)\n",
    "\n",
    "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "  loss_ *= mask\n",
    "\n",
    "  return tf.reduce_sum(loss_)/tf.reduce_sum(mask) #padding을 제외한 loss\n",
    "\n",
    "\n",
    "def accuracy_function(y_true, y_pred):\n",
    "  y_hat=tf.cast(tf.argmax(y_pred,axis=2), dtype=tf.float32) #y_true가 float32이기 때문\n",
    "  accuracies = tf.equal(y_true, y_hat)\n",
    "  #accuracies = tf.equal(y_true, tf.argmax(y_pred, axis=2)) #True, False로 출력\n",
    "\n",
    "  mask = tf.math.logical_not(tf.math.equal(y_true, 0))\n",
    "  accuracies = tf.math.logical_and(mask, accuracies) #둘다 true일 때만 True\n",
    "\n",
    "  accuracies = tf.cast(accuracies, dtype=tf.float32)\n",
    "  mask = tf.cast(mask, dtype=tf.float32)\n",
    "  return tf.reduce_sum(accuracies)/tf.reduce_sum(mask)\n",
    "\n",
    "#def accuracy(y_true,y_pred):\n",
    "#    return tf.keras.metrics.sparse_categorical_accuracy(y_true, y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "data_set=tf.data.Dataset.from_tensor_slices(((encoder_input_data,decoder_input_data),decoder_target_data))\n",
    "data_final=data_set.shuffle(1000).batch(64).prefetch(64)#memory를 차지하지 않음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "782/782 [==============================] - 25s 29ms/step - loss: 2.4657 - accuracy_function: 0.5192\n",
      "Epoch 2/30\n",
      "782/782 [==============================] - 26s 33ms/step - loss: 1.3062 - accuracy_function: 0.6737\n",
      "Epoch 3/30\n",
      "782/782 [==============================] - 26s 33ms/step - loss: 1.1579 - accuracy_function: 0.6992\n",
      "Epoch 4/30\n",
      "782/782 [==============================] - 27s 34ms/step - loss: 1.0805 - accuracy_function: 0.7118\n",
      "Epoch 5/30\n",
      "782/782 [==============================] - 27s 34ms/step - loss: 1.0394 - accuracy_function: 0.7184\n",
      "Epoch 6/30\n",
      "782/782 [==============================] - 31s 40ms/step - loss: 1.0047 - accuracy_function: 0.7242\n",
      "Epoch 7/30\n",
      "782/782 [==============================] - 38s 48ms/step - loss: 0.9784 - accuracy_function: 0.7301\n",
      "Epoch 8/30\n",
      "782/782 [==============================] - 30s 38ms/step - loss: 0.9533 - accuracy_function: 0.7338\n",
      "Epoch 9/30\n",
      "782/782 [==============================] - 26s 34ms/step - loss: 0.9366 - accuracy_function: 0.7364\n",
      "Epoch 10/30\n",
      "782/782 [==============================] - 28s 36ms/step - loss: 0.9175 - accuracy_function: 0.7407\n",
      "Epoch 11/30\n",
      "782/782 [==============================] - 30s 39ms/step - loss: 0.9093 - accuracy_function: 0.7417\n",
      "Epoch 12/30\n",
      "782/782 [==============================] - 36s 46ms/step - loss: 0.8932 - accuracy_function: 0.7447\n",
      "Epoch 13/30\n",
      "782/782 [==============================] - 40s 51ms/step - loss: 0.8845 - accuracy_function: 0.7482\n",
      "Epoch 14/30\n",
      "782/782 [==============================] - 28s 36ms/step - loss: 0.8765 - accuracy_function: 0.7496\n",
      "Epoch 15/30\n",
      "782/782 [==============================] - 27s 35ms/step - loss: 0.8652 - accuracy_function: 0.7509\n",
      "Epoch 16/30\n",
      "782/782 [==============================] - 29s 37ms/step - loss: 0.8558 - accuracy_function: 0.7509\n",
      "Epoch 17/30\n",
      "782/782 [==============================] - 33s 43ms/step - loss: 0.8461 - accuracy_function: 0.7553\n",
      "Epoch 18/30\n",
      "782/782 [==============================] - 44s 57ms/step - loss: 0.8435 - accuracy_function: 0.7541\n",
      "Epoch 19/30\n",
      "782/782 [==============================] - 32s 40ms/step - loss: 0.8361 - accuracy_function: 0.7557\n",
      "Epoch 20/30\n",
      "782/782 [==============================] - 27s 34ms/step - loss: 0.8276 - accuracy_function: 0.7579\n",
      "Epoch 21/30\n",
      "782/782 [==============================] - 29s 36ms/step - loss: 0.8249 - accuracy_function: 0.7582\n",
      "Epoch 22/30\n",
      "782/782 [==============================] - 32s 41ms/step - loss: 0.8182 - accuracy_function: 0.7592\n",
      "Epoch 23/30\n",
      "782/782 [==============================] - 41s 53ms/step - loss: 0.8134 - accuracy_function: 0.7604\n",
      "Epoch 24/30\n",
      "782/782 [==============================] - 37s 48ms/step - loss: 0.8166 - accuracy_function: 0.7589\n",
      "Epoch 25/30\n",
      "782/782 [==============================] - 28s 35ms/step - loss: 0.8027 - accuracy_function: 0.7631\n",
      "Epoch 26/30\n",
      "782/782 [==============================] - 28s 36ms/step - loss: 0.8005 - accuracy_function: 0.7633\n",
      "Epoch 27/30\n",
      "782/782 [==============================] - 31s 39ms/step - loss: 0.7979 - accuracy_function: 0.7633\n",
      "Epoch 28/30\n",
      "782/782 [==============================] - 37s 47ms/step - loss: 0.7971 - accuracy_function: 0.7629\n",
      "Epoch 29/30\n",
      "782/782 [==============================] - 43s 55ms/step - loss: 0.7939 - accuracy_function: 0.7656\n",
      "Epoch 30/30\n",
      "782/782 [==============================] - 29s 37ms/step - loss: 0.7827 - accuracy_function: 0.7675\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x11179c91490>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model=transformer(num_layers=1,d_model=128,num_heads=4,dff=256, \n",
    "                  input_vocab_size=386, target_vocab_size=359,ts_input=8,ts_target=17,rate=0.2)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "model.compile(optimizer=optimizer, loss=loss_function, metrics=[accuracy_function])\n",
    "model.fit(data_final,epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
