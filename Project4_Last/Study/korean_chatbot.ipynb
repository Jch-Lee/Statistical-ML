{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'kcbert': 'beomi@github 님이 만드신 KcBERT 학습데이터',\n",
       " 'korean_chatbot_data': 'songys@github 님이 만드신 챗봇 문답 데이터',\n",
       " 'korean_hate_speech': '{inmoonlight,warnikchow,beomi}@github 님이 만드신 혐오댓글데이터',\n",
       " 'korean_parallel_koen_news': 'jungyeul@github 님이 만드신 병렬 말뭉치',\n",
       " 'korean_petitions': 'lovit@github 님이 만드신 2017.08 ~ 2019.03 청와대 청원데이터',\n",
       " 'kornli': 'KakaoBrain 에서 제공하는 Natural Language Inference (NLI) 데이터',\n",
       " 'korsts': 'KakaoBrain 에서 제공하는 Semantic Textual Similarity (STS) 데이터',\n",
       " 'kowikitext': 'lovit@github 님이 만드신 wikitext 형식의 한국어 위키피디아 데이터',\n",
       " 'namuwikitext': 'lovit@github 님이 만드신 wikitext 형식의 나무위키 데이터',\n",
       " 'naver_changwon_ner': '네이버 + 창원대 NER shared task data',\n",
       " 'nsmc': 'e9t@github 님이 만드신 Naver sentiment movie corpus v1.0',\n",
       " 'question_pair': 'songys@github 님이 만드신 질문쌍(Paired Question v.2)',\n",
       " 'modu_news': '국립국어원에서 만든 모두의 말뭉치: 뉴스 말뭉치',\n",
       " 'modu_messenger': '국립국어원에서 만든 모두의 말뭉치: 메신저 말뭉치',\n",
       " 'modu_mp': '국립국어원에서 만든 모두의 말뭉치: 형태 분석 말뭉치',\n",
       " 'modu_ne': '국립국어원에서 만든 모두의 말뭉치: 개체명 분석 말뭉치',\n",
       " 'modu_spoken': '국립국어원에서 만든 모두의 말뭉치: 구어 말뭉치',\n",
       " 'modu_web': '국립국어원에서 만든 모두의 말뭉치: 웹 말뭉치',\n",
       " 'modu_written': '국립국어원에서 만든 모두의 말뭉치: 문어 말뭉치',\n",
       " 'open_subtitles': 'Open parallel corpus (OPUS) 에서 제공하는 영화 자막 번역 병렬 말뭉치',\n",
       " 'aihub_translation': 'AI Hub 에서 제공하는 번역용 병렬 말뭉치 (구어 + 대화 + 뉴스 + 한국문화 + 조례 + 지자체웹사이트)',\n",
       " 'aihub_spoken_translation': 'AI Hub 에서 제공하는 번역용 병렬 말뭉치 (구어)',\n",
       " 'aihub_conversation_translation': 'AI Hub 에서 제공하는 번역용 병렬 말뭉치 (대화)',\n",
       " 'aihub_news_translation': 'AI Hub 에서 제공하는 번역용 병렬 말뭉치 (뉴스)',\n",
       " 'aihub_korean_culture_translation': 'AI Hub 에서 제공하는 번역용 병렬 말뭉치 (한국문화)',\n",
       " 'aihub_decree_translation': 'AI Hub 에서 제공하는 번역용 병렬 말뭉치 (조례)',\n",
       " 'aihub_government_website_translation': 'AI Hub 에서 제공하는 번역용 병렬 말뭉치 (지자체웹사이트)'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!pip install korpora\n",
    "from Korpora import Korpora\n",
    "Korpora.corpus_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Korpora 는 다른 분들이 연구 목적으로 공유해주신 말뭉치들을\n",
      "    손쉽게 다운로드, 사용할 수 있는 기능만을 제공합니다.\n",
      "\n",
      "    말뭉치들을 공유해 주신 분들에게 감사드리며, 각 말뭉치 별 설명과 라이센스를 공유 드립니다.\n",
      "    해당 말뭉치에 대해 자세히 알고 싶으신 분은 아래의 description 을 참고,\n",
      "    해당 말뭉치를 연구/상용의 목적으로 이용하실 때에는 아래의 라이센스를 참고해 주시기 바랍니다.\n",
      "\n",
      "    # Description\n",
      "    Author : songys@github\n",
      "    Repository : https://github.com/songys/Question_pair\n",
      "    References :\n",
      "\n",
      "    질문쌍(Paired Question v.2)\n",
      "    짝 지어진 두 개의 질문이 같은 질문인지 다른 질문인지 핸드 레이블을 달아둔 데이터\n",
      "    사랑, 이별, 또는 일상과 같은 주제로 도메인 특정적이지 않음\n",
      "\n",
      "    # License\n",
      "    Creative Commons Attribution-ShareAlike license (CC BY-SA 4.0)\n",
      "    Details in https://creativecommons.org/licenses/by-sa/4.0/\n",
      "\n",
      "[Korpora] Corpus `question_pair` is already installed at C:\\Users\\user\\Korpora\\question_pair\\kor_pair_train.csv\n",
      "[Korpora] Corpus `question_pair` is already installed at C:\\Users\\user\\Korpora\\question_pair\\kor_pair_test.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Korpora.korpus_question_pair.QuestionPairKorpus at 0x2d498f7e160>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Korpora.load('question_pair')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "국립국어원에서 API 기능을 제공해 줄 수 없음을 확인하였습니다.\n이에 따라 모두의 말뭉치는 fetch 기능을 제공하지 않습니다",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-202688cdd405>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mKorpora\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'modu_news'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mforce_download\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\Korpora\\loader.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(cls, corpus_names, root_dir, force_download)\u001b[0m\n\u001b[0;32m     45\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mreturn_single\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m             \u001b[0mcorpus_names\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mcorpus_names\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 47\u001b[1;33m         \u001b[0mcorpora\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mKORPUS\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcorpus_name\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mroot_dir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mforce_download\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mcorpus_name\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcorpus_names\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     48\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mreturn_single\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mcorpora\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\Korpora\\loader.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     45\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mreturn_single\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m             \u001b[0mcorpus_names\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mcorpus_names\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 47\u001b[1;33m         \u001b[0mcorpora\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mKORPUS\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcorpus_name\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mroot_dir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mforce_download\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mcorpus_name\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcorpus_names\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     48\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mreturn_single\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mcorpora\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\Korpora\\korpus_modu_news.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, root_dir, force_download, load_light)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mModuNewsKorpus\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mModuKorpus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mroot_dir\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mforce_download\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mload_light\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 54\u001b[1;33m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mforce_download\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     55\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mroot_dir\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m             \u001b[0mroot_dir\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdefault_korpora_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'NIKL_NEWSPAPER'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\Korpora\\korpus_modu_news.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, force_download)\u001b[0m\n\u001b[0;32m     46\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mforce_download\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mforce_download\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 48\u001b[1;33m             \u001b[0mfetch_modu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     49\u001b[0m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdescription\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlicense\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\Korpora\\korpus_modu_news.py\u001b[0m in \u001b[0;36mfetch_modu\u001b[1;34m(root_dir, force_download)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    178\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mfetch_modu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mroot_dir\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mforce_download\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 179\u001b[1;33m     raise NotImplementedError(\n\u001b[0m\u001b[0;32m    180\u001b[0m         \u001b[1;34m\"국립국어원에서 API 기능을 제공해 줄 수 없음을 확인하였습니다.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    181\u001b[0m         \u001b[1;34m\"\\n이에 따라 모두의 말뭉치는 fetch 기능을 제공하지 않습니다\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNotImplementedError\u001b[0m: 국립국어원에서 API 기능을 제공해 줄 수 없음을 확인하였습니다.\n이에 따라 모두의 말뭉치는 fetch 기능을 제공하지 않습니다"
     ]
    }
   ],
   "source": [
    "Korpora.load('modu_news', force_download=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Korpora 는 다른 분들이 연구 목적으로 공유해주신 말뭉치들을\n",
      "    손쉽게 다운로드, 사용할 수 있는 기능만을 제공합니다.\n",
      "\n",
      "    말뭉치들을 공유해 주신 분들에게 감사드리며, 각 말뭉치 별 설명과 라이센스를 공유 드립니다.\n",
      "    해당 말뭉치에 대해 자세히 알고 싶으신 분은 아래의 description 을 참고,\n",
      "    해당 말뭉치를 연구/상용의 목적으로 이용하실 때에는 아래의 라이센스를 참고해 주시기 바랍니다.\n",
      "\n",
      "    # Description\n",
      "    Author : songys@github\n",
      "    Repository : https://github.com/songys/Chatbot_data\n",
      "    References :\n",
      "\n",
      "    Chatbot_data_for_Korean v1.0\n",
      "      1. 챗봇 트레이닝용 문답 페어 11,876개\n",
      "      2. 일상다반사 0, 이별(부정) 1, 사랑(긍정) 2로 레이블링\n",
      "    자세한 내용은 위의 repository를 참고하세요.\n",
      "\n",
      "    # License\n",
      "    CC0 1.0 Universal (CC0 1.0) Public Domain Dedication\n",
      "    Details in https://creativecommons.org/publicdomain/zero/1.0/\n",
      "\n"
     ]
    },
    {
     "ename": "HTTPError",
     "evalue": "HTTP Error 404: Not Found",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-cd7c852918ac>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mKorpora\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'korean_chatbot_data'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mforce_download\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\Korpora\\loader.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(cls, corpus_names, root_dir, force_download)\u001b[0m\n\u001b[0;32m     45\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mreturn_single\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m             \u001b[0mcorpus_names\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mcorpus_names\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 47\u001b[1;33m         \u001b[0mcorpora\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mKORPUS\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcorpus_name\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mroot_dir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mforce_download\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mcorpus_name\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcorpus_names\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     48\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mreturn_single\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mcorpora\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\Korpora\\loader.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     45\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mreturn_single\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m             \u001b[0mcorpus_names\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mcorpus_names\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 47\u001b[1;33m         \u001b[0mcorpora\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mKORPUS\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcorpus_name\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mroot_dir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mforce_download\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mcorpus_name\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcorpus_names\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     48\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mreturn_single\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mcorpora\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\Korpora\\korpus_chatbot_data.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, root_dir, force_download)\u001b[0m\n\u001b[0;32m     42\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mroot_dir\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m             \u001b[0mroot_dir\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdefault_korpora_path\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m         \u001b[0mfetch_chatbot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mroot_dir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mforce_download\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m         \u001b[0mlocal_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mabspath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mroot_dir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mKOREAN_CHATBOT_FETCH_INFORMATION\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'destination'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\Korpora\\korpus_chatbot_data.py\u001b[0m in \u001b[0;36mfetch_chatbot\u001b[1;34m(root_dir, force_download)\u001b[0m\n\u001b[0;32m     71\u001b[0m         \u001b[0mdestination\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minformation\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'destination'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m         \u001b[0mlocal_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mabspath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mroot_dir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdestination\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 73\u001b[1;33m         \u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlocal_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'korean_chatbot_data'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mforce_download\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minformation\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'method'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\Korpora\\utils.py\u001b[0m in \u001b[0;36mfetch\u001b[1;34m(remote_path, local_path, corpus_name, force_download, method)\u001b[0m\n\u001b[0;32m    213\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    214\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"download\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 215\u001b[1;33m         \u001b[0mweb_download\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mremote_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdestination\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcorpus_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mforce_download\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    216\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"google_drive\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    217\u001b[0m         \u001b[0mgoogle_drive_download\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mremote_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdestination\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcorpus_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mforce_download\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\Korpora\\utils.py\u001b[0m in \u001b[0;36mweb_download\u001b[1;34m(url, local_path, corpus_name, force_download)\u001b[0m\n\u001b[0;32m    108\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mweb_download\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlocal_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcorpus_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mforce_download\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 110\u001b[1;33m     \u001b[0msite\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrequest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0murlopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    111\u001b[0m     \u001b[0mmeta\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msite\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    112\u001b[0m     \u001b[0mremote_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmeta\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Content-Length'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36murlopen\u001b[1;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[0;32m    220\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    221\u001b[0m         \u001b[0mopener\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_opener\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 222\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mopener\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    223\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    224\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0minstall_opener\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopener\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36mopen\u001b[1;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[0;32m    529\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mprocessor\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprocess_response\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    530\u001b[0m             \u001b[0mmeth\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocessor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 531\u001b[1;33m             \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmeth\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    532\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    533\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36mhttp_response\u001b[1;34m(self, request, response)\u001b[0m\n\u001b[0;32m    638\u001b[0m         \u001b[1;31m# request was successfully received, understood, and accepted.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    639\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m200\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[0mcode\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m300\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 640\u001b[1;33m             response = self.parent.error(\n\u001b[0m\u001b[0;32m    641\u001b[0m                 'http', request, response, code, msg, hdrs)\n\u001b[0;32m    642\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36merror\u001b[1;34m(self, proto, *args)\u001b[0m\n\u001b[0;32m    567\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mhttp_err\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    568\u001b[0m             \u001b[0margs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'default'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'http_error_default'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0morig_args\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 569\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_chain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    570\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    571\u001b[0m \u001b[1;31m# XXX probably also want an abstract factory that knows when it makes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36m_call_chain\u001b[1;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[0;32m    500\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhandler\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mhandlers\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    501\u001b[0m             \u001b[0mfunc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandler\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 502\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    503\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    504\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36mhttp_error_default\u001b[1;34m(self, req, fp, code, msg, hdrs)\u001b[0m\n\u001b[0;32m    647\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mHTTPDefaultErrorHandler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBaseHandler\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    648\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mhttp_error_default\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhdrs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 649\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfull_url\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhdrs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    650\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    651\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mHTTPRedirectHandler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBaseHandler\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mHTTPError\u001b[0m: HTTP Error 404: Not Found"
     ]
    }
   ],
   "source": [
    "Korpora.load('korean_chatbot_data',force_download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "chat_data=pd.read_csv('C:/Users/user/Korpora/korean_chatbot_data/ChatbotData.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Q            A  label\n",
      "0           12시 땡!   하루가 또 가네요.      0\n",
      "1      1지망 학교 떨어졌어    위로해 드립니다.      0\n",
      "2     3박4일 놀러가고 싶다  여행은 언제나 좋죠.      0\n",
      "3  3박4일 정도 놀러가고 싶다  여행은 언제나 좋죠.      0\n",
      "4          PPL 심하네   눈살이 찌푸려지죠.      0\n"
     ]
    }
   ],
   "source": [
    "print(chat_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11823, 3)\n"
     ]
    }
   ],
   "source": [
    "print(chat_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Q           A  label\n",
      "0            12시 땡   하루가 또 가네요      0\n",
      "1      1지망 학교 떨어졌어    위로해 드립니다      0\n",
      "2     3박4일 놀러가고 싶다  여행은 언제나 좋죠      0\n",
      "3  3박4일 정도 놀러가고 싶다  여행은 언제나 좋죠      0\n",
      "4          PPL 심하네   눈살이 찌푸려지죠      0\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "exclude=set(string.punctuation)\n",
    "chat_data.Q=chat_data.Q.apply(lambda x:''.join (ch for ch in x if ch not in exclude))\n",
    "chat_data.A=chat_data.A.apply(lambda x:''.join (ch for ch in x if ch not in exclude))\n",
    "print(chat_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8510\n"
     ]
    }
   ],
   "source": [
    "#!pip install tensorflow_datasets\n",
    "import tensorflow_datasets as tfds\n",
    "tokenizer = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(\n",
    "    chat_data.Q+chat_data.A, target_vocab_size=2**13)\n",
    "print(tokenizer.vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5871, 590, 3523, 127, 695, 3756, 833]\n",
      "가스비 비싼데 감기 걸리겠어\n"
     ]
    }
   ],
   "source": [
    "ex_1=tokenizer.encode(chat_data.loc[20,'Q'])\n",
    "print(ex_1)\n",
    "ex_2=tokenizer.decode(ex_1)\n",
    "print(ex_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8512\n"
     ]
    }
   ],
   "source": [
    "START_TOKEN, END_TOKEN = [tokenizer.vocab_size], [tokenizer.vocab_size + 1]\n",
    "\n",
    "VOCAB_SIZE = tokenizer.vocab_size + 2 #2개의 token이 추가됨\n",
    "print(VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[8251, 1153, 3075], [8303, 47, 906, 8286, 976, 1715], [8305, 1433, 4683, 8286, 3666, 87], [8305, 1433, 4683, 8286, 1284, 3666, 87], [8334, 8334, 8330, 8286, 4213]]\n",
      "[[8510, 3857, 64, 8230, 8511], [8510, 1824, 5586, 8511], [8510, 3409, 762, 120, 8511], [8510, 3409, 762, 120, 8511], [8510, 964, 2300, 1490, 2180, 4436, 40, 8511]]\n"
     ]
    }
   ],
   "source": [
    "tokenized_inputs, tokenized_outputs = [], []\n",
    "\n",
    "for (sentence1, sentence2) in zip(chat_data.Q, chat_data.A):\n",
    "    # decoder(토큰화 + 정수 인코딩), 시작 토큰과 종료 토큰 추가\n",
    "    sentence1 = tokenizer.encode(sentence1)\n",
    "    sentence2 = START_TOKEN + tokenizer.encode(sentence2) + END_TOKEN\n",
    "\n",
    "    tokenized_inputs.append(sentence1)\n",
    "    tokenized_outputs.append(sentence2) \n",
    "\n",
    "print(tokenized_inputs[:5])\n",
    "print(tokenized_outputs[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n",
      "29\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "lenth_list=[]\n",
    "for l in tokenized_inputs:\n",
    "    lenth_list.append(len(l))\n",
    "q_lenth_max=np.max(lenth_list)\n",
    "print(q_lenth_max)\n",
    "\n",
    "lenth_list=[]\n",
    "for l in tokenized_outputs:\n",
    "    lenth_list.append(len(l))\n",
    "a_lenth_max=np.max(lenth_list)\n",
    "print(a_lenth_max)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "max_length_q=20\n",
    "questions = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "      tokenized_inputs, maxlen=max_length_q, padding='post')\n",
    "\n",
    "max_length_a=29\n",
    "answers = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "      tokenized_outputs, maxlen=max_length_a, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[8251 1153 3075    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0]\n",
      " [8303   47  906 8286  976 1715    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0]]\n",
      "[[8510 3857   64 8230 8511    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [8510 1824 5586 8511    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0]]\n"
     ]
    }
   ],
   "source": [
    "print(questions[:2])\n",
    "print(answers[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12시 땡\n",
      "(11823, 20)\n",
      "(11823, 29)\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(questions[0]))\n",
    "print(questions.shape)\n",
    "print(answers.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[32 20]\n",
      "(32, 28)\n",
      "[32 28]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "data_set=tf.data.Dataset.from_tensor_slices(((questions,answers[:,:-1]),answers[:,1:]))\n",
    "data_final=data_set.shuffle(500).batch(32).prefetch(32)\n",
    "ds_test=next(iter(data_final))\n",
    "print(np.array(ds_test[0][0].shape))\n",
    "print(np.array(ds_test[1]).shape)\n",
    "print(np.array(ds_test[0][1].shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "def get_angles(pos, i, d_model):\n",
    "  angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))\n",
    "  return pos * angle_rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_encoding(position, d_model):\n",
    "  angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n",
    "                          np.arange(d_model)[np.newaxis, :],\n",
    "                          d_model)\n",
    "\n",
    "  # apply sin to even indices in the array; 2i\n",
    "  angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "\n",
    "  # apply cos to odd indices in the array; 2i+1\n",
    "  angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "\n",
    "  pos_encoding = angle_rads[np.newaxis, ...]\n",
    "\n",
    "  return tf.cast(pos_encoding, dtype=tf.float32)#(1,position,d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_padding_mask(x):\n",
    "  mask = tf.cast(tf.math.equal(x, 0), tf.float32)\n",
    "  #x:tokenized words with padding for each sentence 1D텐서 [...,0,0,0]\n",
    "  # add extra dimensions to add the padding\n",
    "  # to the attention logits.\n",
    "  return mask[:, tf.newaxis, tf.newaxis, :]  # (batch_size, 1, 1, seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_look_ahead_mask(x):\n",
    "    seq_len=tf.shape(x)[1]\n",
    "    look_ahead_mask=1-tf.linalg.band_part(tf.ones((seq_len,seq_len)),-1,0)\n",
    "    padding_mask=create_padding_mask(x)\n",
    "    return tf.maximum(look_ahead_mask,padding_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(q, k, v, mask):\n",
    "  \"\"\"Calculate the attention weights.\n",
    "  q, k, v must have matching leading dimensions.\n",
    "  k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v.\n",
    "  The mask has different shapes depending on its type(padding or look ahead)\n",
    "  but it must be broadcastable for addition.\n",
    "\n",
    "  Args:\n",
    "    q: query shape == (..., seq_len_q, depth) (number of heads,ts,64)\n",
    "    k: key shape == (..., seq_len_k, depth)   (number of heads,ts,64)\n",
    "    v: value shape == (..., seq_len_v, depth_v) (number of heads,ts,64)\n",
    "    mask: Float tensor with shape broadcastable  (1,1,ts) or (ts,ts)\n",
    "          to (..., seq_len_q, seq_len_k). Defaults to None.\n",
    "\n",
    "  Returns:\n",
    "    output, attention_weights\n",
    "  \"\"\"\n",
    "\n",
    "  matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n",
    "\n",
    "  # scale matmul_qk\n",
    "  dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
    "  scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
    "\n",
    "  # add the mask to the scaled tensor.\n",
    "  if mask is not None:\n",
    "    scaled_attention_logits += (mask * -1e9)\n",
    "\n",
    "  # softmax is normalized on the last axis (seq_len_k) so that the scores\n",
    "  # add up to 1.\n",
    "  attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k) (number of heads,ts,ts)\n",
    "\n",
    "  output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v) (number of heads,ts,64)\n",
    "\n",
    "  return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "  def __init__(self, d_model, num_heads):\n",
    "    super(MultiHeadAttention, self).__init__()\n",
    "    self.num_heads = num_heads  #8\n",
    "    self.d_model = d_model #512\n",
    "\n",
    "    assert d_model % self.num_heads == 0\n",
    "\n",
    "    self.depth = d_model // self.num_heads\n",
    "\n",
    "    self.wq = tf.keras.layers.Dense(d_model) #(ts,512)\n",
    "    self.wk = tf.keras.layers.Dense(d_model) #(ts,512)\n",
    "    self.wv = tf.keras.layers.Dense(d_model) #(ts,512)\n",
    "\n",
    "    self.dense = tf.keras.layers.Dense(d_model) # scale_dot_attention후 concatenate한 attention (ts,512)를 Dense에 통과하기 위함\n",
    "\n",
    "  def split_heads(self, x, batch_size):\n",
    "    \"\"\"Split the last dimension into (num_heads, depth).\n",
    "    Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
    "    (batch,ts,512)를 (batch,ts,heads(8), depth(64))로 reshape한후 (batch,heads,ts,depth)로 반환하는 함수\n",
    "    \"\"\"\n",
    "    x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
    "    return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "\n",
    "  def call(self, v, k, q, mask):\n",
    "    batch_size = tf.shape(q)[0]\n",
    "\n",
    "    q = self.wq(q)  # (batch_size, ts, 512) 여기에서 512=d_model=embedding_size\n",
    "    k = self.wk(k)  # (batch_size, ts, 512)\n",
    "    v = self.wv(v)  # (batch_size, ts, 512)\n",
    "\n",
    "    q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth) depth=64, num_heads=8,seq_len_q=ts \n",
    "    k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth) depth=64, num_heads=8,seq_len_k=ts \n",
    "    v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth) depth=64, num_heads=8,seq_len_v=ts \n",
    "\n",
    "    # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n",
    "    # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n",
    "    scaled_attention, attention_weights = scaled_dot_product_attention(\n",
    "        q, k, v, mask)\n",
    "\n",
    "    scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, depth)\n",
    "\n",
    "    concat_attention = tf.reshape(scaled_attention,\n",
    "                                  (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n",
    "\n",
    "    output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n",
    "\n",
    "    return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def point_wise_feed_forward_network(d_model, dff):\n",
    "  return tf.keras.Sequential([\n",
    "      tf.keras.layers.Dense(dff, activation='relu'),  # (batch_size, seq_len, dff) seq_len=ts\n",
    "      tf.keras.layers.Dense(d_model)  # (batch_size, seq_len, d_model) d_model=512\n",
    "  ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "  def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "    super(EncoderLayer, self).__init__()\n",
    "\n",
    "    self.mha = MultiHeadAttention(d_model, num_heads)\n",
    "    self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "\n",
    "    self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "    self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "    self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "    self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "  def call(self, x, mask):\n",
    "\n",
    "    attn_output, _ = self.mha(x, x, x, mask)  # (batch_size, input_seq_len, d_model) input_seq_len=input_ts, d_model=512\n",
    "    attn_output = self.dropout1(attn_output)\n",
    "    out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n",
    "\n",
    "    ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n",
    "    ffn_output = self.dropout2(ffn_output)\n",
    "    out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n",
    "\n",
    "    return out2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "  def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "    super(DecoderLayer, self).__init__()\n",
    "\n",
    "    self.mha1 = MultiHeadAttention(d_model, num_heads)\n",
    "    self.mha2 = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "    self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "\n",
    "    self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "    self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "    self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "    self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "    self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "    self.dropout3 = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "  def call(self, x, enc_output, look_ahead_mask, padding_mask):\n",
    "    # enc_output.shape == (batch_size, input_seq_len, d_model) x: decoder input\n",
    "\n",
    "    attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask)  # (batch_size, target_seq_len, d_model) \n",
    "    attn1 = self.dropout1(attn1)                   #targer_seq_len=decoder_input_ts\n",
    "    out1 = self.layernorm1(attn1 + x)\n",
    "\n",
    "    attn2, attn_weights_block2 = self.mha2(enc_output, enc_output, out1, padding_mask) \n",
    "    attn2 = self.dropout2(attn2)\n",
    "    out2 = self.layernorm2(attn2 + out1)  # (batch_size, target_seq_len, d_model) #target_seq_len=decoder_input_ts\n",
    "\n",
    "    ffn_output = self.ffn(out2)  # (batch_size, target_seq_len, d_model)\n",
    "    ffn_output = self.dropout3(ffn_output)\n",
    "    out3 = self.layernorm3(ffn_output + out2)  # (batch_size, target_seq_len, d_model)\n",
    "\n",
    "    return out3, attn_weights_block1, attn_weights_block2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.layers.Layer):\n",
    "  def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size,\n",
    "               maximum_position_encoding, rate=0.1):\n",
    "    super(Encoder, self).__init__()\n",
    "\n",
    "    self.d_model = d_model\n",
    "    self.num_layers = num_layers\n",
    "\n",
    "    self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)\n",
    "    self.pos_encoding = positional_encoding(maximum_position_encoding,\n",
    "                                            self.d_model)\n",
    "\n",
    "    self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate)\n",
    "                       for _ in range(num_layers)]\n",
    "\n",
    "    self.dropout = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "  def call(self, x, mask):\n",
    "\n",
    "    seq_len = tf.shape(x)[1]  #x는 tokenized sentence with padding\n",
    "\n",
    "    # adding embedding and position encoding.\n",
    "    x = self.embedding(x)  # (batch_size, input_seq_len, d_model)\n",
    "    x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "    x += self.pos_encoding[:, :seq_len, :]\n",
    "\n",
    "    x = self.dropout(x)\n",
    "\n",
    "    for i in range(self.num_layers):\n",
    "      x = self.enc_layers[i](x, mask)\n",
    "\n",
    "    return x  # (batch_size, input_seq_len, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.layers.Layer):\n",
    "  def __init__(self, num_layers, d_model, num_heads, dff, target_vocab_size,\n",
    "               maximum_position_encoding, rate=0.1):\n",
    "    super(Decoder, self).__init__()\n",
    "\n",
    "    self.d_model = d_model\n",
    "    self.num_layers = num_layers\n",
    "\n",
    "    self.embedding = tf.keras.layers.Embedding(target_vocab_size, d_model)\n",
    "    self.pos_encoding = positional_encoding(maximum_position_encoding, d_model)\n",
    "\n",
    "    self.dec_layers = [DecoderLayer(d_model, num_heads, dff, rate)\n",
    "                       for _ in range(num_layers)]\n",
    "    self.dropout = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "  def call(self, x, enc_output, look_ahead_mask, padding_mask):\n",
    "\n",
    "    seq_len = tf.shape(x)[1]\n",
    "    attention_weights = {}\n",
    "\n",
    "    x = self.embedding(x)  # (batch_size, target_seq_len, d_model)\n",
    "    x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "    x += self.pos_encoding[:, :seq_len, :]\n",
    "\n",
    "    x = self.dropout(x)\n",
    "\n",
    "    for i in range(self.num_layers):\n",
    "      x, block1, block2 = self.dec_layers[i](x, enc_output,look_ahead_mask, padding_mask)\n",
    "\n",
    "      attention_weights[f'decoder_layer{i+1}_block1'] = block1\n",
    "      attention_weights[f'decoder_layer{i+1}_block2'] = block2\n",
    "\n",
    "    # x.shape == (batch_size, target_seq_len, d_model)\n",
    "    return x, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformer(num_layers, d_model, num_heads, dff, input_vocab_size,\n",
    "               target_vocab_size, ts_input, ts_target, rate=0.1):\n",
    "    \n",
    "    enc_input = tf.keras.Input(shape=(None,))  \n",
    "    dec_input = tf.keras.Input(shape=(None,))\n",
    "    \n",
    "    enc_padding_mask=tf.keras.layers.Lambda(create_padding_mask,output_shape=(1,1,None))(enc_input)\n",
    "    look_ahead_mask=tf.keras.layers.Lambda(create_look_ahead_mask,output_shape=(1,None,None))(dec_input)\n",
    "    dec_padding_mask=tf.keras.layers.Lambda(create_padding_mask,output_shape=(1,1,None))(enc_input)\n",
    "    \n",
    "    enc_output=Encoder(num_layers, d_model, num_heads, dff,\n",
    "                             input_vocab_size, ts_input, rate)(enc_input,enc_padding_mask)\n",
    "    dec_output,_ =Decoder(num_layers, d_model, num_heads, dff,\n",
    "                           target_vocab_size, ts_target, rate)(dec_input,enc_output,look_ahead_mask,dec_padding_mask)\n",
    "    outputs=tf.keras.layers.Dense(target_vocab_size)(dec_output)\n",
    "    \n",
    "    return tf.keras.Model(inputs=[enc_input, dec_input], outputs=outputs)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(y_true, y_pred):\n",
    "  mask = tf.math.logical_not(tf.math.equal(y_true, 0))\n",
    "  loss_ = loss_object(y_true, y_pred)\n",
    "\n",
    "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "  loss_ *= mask\n",
    "\n",
    "  return tf.reduce_sum(loss_)/tf.reduce_sum(mask) #padding을 제외한 loss\n",
    "\n",
    "\n",
    "def accuracy_function(y_true, y_pred):\n",
    "  y_hat=tf.cast(tf.argmax(y_pred,axis=2), dtype=tf.float32) #y_true가 float32이기 때문\n",
    "  accuracies = tf.equal(y_true, y_hat)\n",
    "  #accuracies = tf.equal(y_true, tf.argmax(y_pred, axis=2)) #True, False로 출력\n",
    "\n",
    "  mask = tf.math.logical_not(tf.math.equal(y_true, 0))\n",
    "  accuracies = tf.math.logical_and(mask, accuracies) #둘다 true일 때만 True\n",
    "\n",
    "  accuracies = tf.cast(accuracies, dtype=tf.float32)\n",
    "  mask = tf.cast(mask, dtype=tf.float32)\n",
    "  return tf.reduce_sum(accuracies)/tf.reduce_sum(mask)\n",
    "\n",
    "def accuracy(y_true,y_pred):\n",
    "    return tf.keras.metrics.sparse_categorical_accuracy(y_true, y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "370/370 [==============================] - 39s 93ms/step - loss: 6.9919 - accuracy_function: 0.1877\n",
      "Epoch 2/30\n",
      "370/370 [==============================] - 39s 105ms/step - loss: 5.1954 - accuracy_function: 0.2635\n",
      "Epoch 3/30\n",
      "370/370 [==============================] - 40s 107ms/step - loss: 3.9610 - accuracy_function: 0.3774\n",
      "Epoch 4/30\n",
      "370/370 [==============================] - 48s 129ms/step - loss: 2.8850 - accuracy_function: 0.4979\n",
      "Epoch 5/30\n",
      "370/370 [==============================] - 62s 168ms/step - loss: 2.0859 - accuracy_function: 0.6097\n",
      "Epoch 6/30\n",
      "370/370 [==============================] - 45s 122ms/step - loss: 1.4856 - accuracy_function: 0.7043\n",
      "Epoch 7/30\n",
      "370/370 [==============================] - 47s 127ms/step - loss: 1.0326 - accuracy_function: 0.7786\n",
      "Epoch 8/30\n",
      "370/370 [==============================] - 60s 163ms/step - loss: 0.7321 - accuracy_function: 0.8317\n",
      "Epoch 9/30\n",
      "370/370 [==============================] - 54s 145ms/step - loss: 0.5460 - accuracy_function: 0.8688\n",
      "Epoch 10/30\n",
      "370/370 [==============================] - 46s 125ms/step - loss: 0.4200 - accuracy_function: 0.8952\n",
      "Epoch 11/30\n",
      "370/370 [==============================] - 59s 159ms/step - loss: 0.3509 - accuracy_function: 0.9106\n",
      "Epoch 12/30\n",
      "370/370 [==============================] - 54s 146ms/step - loss: 0.2918 - accuracy_function: 0.9240\n",
      "Epoch 13/30\n",
      "370/370 [==============================] - 46s 125ms/step - loss: 0.2687 - accuracy_function: 0.9286\n",
      "Epoch 14/30\n",
      "370/370 [==============================] - 60s 162ms/step - loss: 0.2349 - accuracy_function: 0.9367\n",
      "Epoch 15/30\n",
      "370/370 [==============================] - 55s 149ms/step - loss: 0.2174 - accuracy_function: 0.9406\n",
      "Epoch 16/30\n",
      "370/370 [==============================] - 46s 125ms/step - loss: 0.2085 - accuracy_function: 0.9440\n",
      "Epoch 17/30\n",
      "370/370 [==============================] - 59s 158ms/step - loss: 0.1972 - accuracy_function: 0.94664s - loss: 0.1931 \n",
      "Epoch 18/30\n",
      "370/370 [==============================] - 57s 153ms/step - loss: 0.1876 - accuracy_function: 0.9491\n",
      "Epoch 19/30\n",
      "370/370 [==============================] - 48s 129ms/step - loss: 0.1818 - accuracy_function: 0.9491\n",
      "Epoch 20/30\n",
      "370/370 [==============================] - 62s 169ms/step - loss: 0.1721 - accuracy_function: 0.9527\n",
      "Epoch 21/30\n",
      "370/370 [==============================] - 50s 135ms/step - loss: 0.1672 - accuracy_function: 0.9532\n",
      "Epoch 22/30\n",
      "370/370 [==============================] - 48s 130ms/step - loss: 0.1614 - accuracy_function: 0.9540\n",
      "Epoch 23/30\n",
      "370/370 [==============================] - 63s 172ms/step - loss: 0.1540 - accuracy_function: 0.9572\n",
      "Epoch 24/30\n",
      "370/370 [==============================] - 52s 140ms/step - loss: 0.1541 - accuracy_function: 0.9568\n",
      "Epoch 25/30\n",
      "370/370 [==============================] - 49s 132ms/step - loss: 0.1504 - accuracy_function: 0.9586\n",
      "Epoch 26/30\n",
      "370/370 [==============================] - 65s 174ms/step - loss: 0.1387 - accuracy_function: 0.9601\n",
      "Epoch 27/30\n",
      "370/370 [==============================] - 48s 130ms/step - loss: 0.1426 - accuracy_function: 0.9598\n",
      "Epoch 28/30\n",
      "370/370 [==============================] - 51s 137ms/step - loss: 0.1355 - accuracy_function: 0.96212s - loss: 0.1338 - accuracy\n",
      "Epoch 29/30\n",
      "370/370 [==============================] - 64s 174ms/step - loss: 0.1293 - accuracy_function: 0.9642\n",
      "Epoch 30/30\n",
      "370/370 [==============================] - 47s 126ms/step - loss: 0.1353 - accuracy_function: 0.9610\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x16e4e9ae250>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model=transformer(num_layers=1,d_model=128,num_heads=4,dff=256, \n",
    "                  input_vocab_size=VOCAB_SIZE-2, target_vocab_size=VOCAB_SIZE,ts_input=20,ts_target=28,rate=0.1)\n",
    "                  #num_layers=2,3,4는 작동안함. 아마도 작으면 encoder_input에 영향을 많이 받도록함\n",
    "                  #num_layers가 크면 encoder보다 decoder_input에 더 큰영향을 받아 첫 word=START_TOKEN으로 같으므로 \n",
    "                  #항상 같은 답변으로 나올 가능성이 많음. 실제로 그러함. embedding size는 작은 것이 우수함 \n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "model.compile(optimizer=optimizer, loss=loss_function, metrics=[accuracy_function])\n",
    "model.fit(data_final,epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(sentence):\n",
    "  sentence_1 = preprocess_sentence(sentence)\n",
    "  token_sentence=[]\n",
    "  t_sentence=tokenizer.encode(sentence_1)\n",
    "  token_sentence.append(t_sentence)\n",
    "  encoder_input=tf.keras.preprocessing.sequence.pad_sequences(token_sentence, maxlen=20, padding='post')\n",
    "  print(encoder_input)\n",
    "  decoder_input_pred=np.zeros((1,max_length_a-1),dtype='int32')\n",
    "  word=START_TOKEN[0]\n",
    "  for i in range(max_length_a-1):\n",
    "     decoder_input_pred[:,i]=word\n",
    "     pred=model.predict((encoder_input,decoder_input_pred))\n",
    "     t=np.argmax(pred[0][i])\n",
    "     word=t\n",
    "     if word == END_TOKEN:\n",
    "          break\n",
    "  print(pred.shape)\n",
    "  print(decoder_input_pred)\n",
    "  \n",
    "  return encoder_input, decoder_input_pred\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(sentence):\n",
    "  encode_input, decode_input = evaluate(sentence)\n",
    "  \n",
    "  t=model.predict([encode_input,decode_input])\n",
    "  prediction=np.argmax(t,axis=2)\n",
    "  print(prediction)\n",
    "  print(prediction.shape)\n",
    "  prediction=tf.squeeze(prediction,axis=0)  \n",
    "  #prediction_sentence=tokenizer.decode(prediction)\n",
    "  \n",
    "  predicted_sentence=[]\n",
    "  for i in prediction:\n",
    "    if i>8510:\n",
    "        break\n",
    "    else:\n",
    "        predicted_sentence.append(tokenizer.decode([i]))\n",
    "    \n",
    "        \n",
    "  #predicted_sentence = tokenizer.decode(\n",
    "    #[i for i in list(prediction) if i < 8511])\n",
    "\n",
    "  print('Input: {}'.format(sentence))\n",
    "  print('Output: {}'.format(predicted_sentence))\n",
    "\n",
    "  return predicted_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "피곤하고 재미없다\n",
      "[[4783   38 6460   70    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0]]\n",
      "(1, 28, 8512)\n",
      "[[8510 3812  108 4860  188 3610 7047  209 1551   59   28    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0]]\n",
      "[[3812  108 4860  188 3610 7047  209 1551   59   28 8511 8286 8286 8286\n",
      "  8286 8286 8286 8286 8286 8286 8286 8286 8286 8286   34   34 8286 8286]]\n",
      "(1, 28)\n",
      "Input: 피곤하고 재미없다.\n",
      "Output: ['활', '기', '찬 ', '사람을 ', '만나보', '시면 ', '생각이 ', '바뀔 ', '수도 ', '있어요']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['활', '기', '찬 ', '사람을 ', '만나보', '시면 ', '생각이 ', '바뀔 ', '수도 ', '있어요']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exclude=set(string.punctuation)\n",
    "def preprocess_sentence(x):\n",
    "    t=[]\n",
    "    for i in range(len(x)):\n",
    "        if x[i] not in exclude:\n",
    "            t.append(x[i])\n",
    "    #print(t)\n",
    "    x3=''\n",
    "    for i in range(len(t)):\n",
    "        x3=x3+t[i]+''\n",
    "    return x3\n",
    "\n",
    "sentence='피곤하고 재미없다.'\n",
    "sentence_1=preprocess_sentence(sentence)\n",
    "print(sentence_1)\n",
    "\n",
    "predict(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[8305 1433 4683 8286 3666   87    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0]]\n",
      "(1, 28, 8512)\n",
      "[[8510 3409  762  120    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0]]\n",
      "[[3409  762  120 8511 8511 8511 8511 8511 8511 8511 8511 8511 8511 8511\n",
      "  8511 8511 8511 8511 8511 8511 8511 8511 8511 8511 8511 8511   17   17]]\n",
      "(1, 28)\n",
      "Input: 3박4일 놀러가고 싶다\n",
      "Output: ['여행은 ', '언제나 ', '좋죠']\n",
      "[[1770  736  396    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0]]\n",
      "(1, 28, 8512)\n",
      "[[8510  321 1031   81    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0]]\n",
      "[[ 321 1031   81 8511 8511 8286 8286 8286 8286 8286 8511 8511 8511 8286\n",
      "    30   30 8286 8286   30   30 8286 8286 8286 8286 8511   30   30   30]]\n",
      "(1, 28)\n",
      "Input: 카페 갈래\n",
      "Output: ['저는 ', '고민이 ', '없어요']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['저는 ', '고민이 ', '없어요']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict('3박4일 놀러가고 싶다')#띄어쓰기에 영향받음\n",
    "predict('카페 갈래')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[8251 1153 3075    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0]]\n",
      "(1, 28, 8512)\n",
      "[[8510 3857   64 8230    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0]]\n",
      "[[3857   64 8230 8511 8511 8511 8511 8511 8511 8511 8511 8511 8511 8511\n",
      "  8511 8511 8511 8511 8511 8511 8511 8511 8511 8511 8511 8511 8230 8230]]\n",
      "(1, 28)\n",
      "Input: 12시 땡\n",
      "Output: ['하루가 ', '또 ', '가네요']\n",
      "[[8334 8334 8330 8286 4213    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0]]\n",
      "(1, 28, 8512)\n",
      "[[8510  964 2300 1490 2180 4436   40    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0]]\n",
      "[[ 964 2300 1490 2180 4436   40 8511  182  182  182  182 7596  201  201\n",
      "   201  182  182  182  182   59 7596 7596 7596 7596 7596 7596 7596  553]]\n",
      "(1, 28)\n",
      "Input: PPL 심하네\n",
      "Output: ['눈', '살이 ', '찌', '푸', '려지', '죠']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['눈', '살이 ', '찌', '푸', '려지', '죠']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict('12시 땡')\n",
    "predict('PPL 심하네')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 486 1437    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0]]\n",
      "(1, 28, 8512)\n",
      "[[8510 5663 1438 3127  272 2696    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0]]\n",
      "[[5663 1438 3127  272 2696 8511 8511 8511 8511  159  159  159  488  272\n",
      "   272  272  159  159    8  128  128  128 8511    8    8    8 7420 7420]]\n",
      "(1, 28)\n",
      "Input: 밥 먹어\n",
      "Output: ['누구랑 ', '먹는 ', '냐에 ', '따라 ', '다르겠죠']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['누구랑 ', '먹는 ', '냐에 ', '따라 ', '다르겠죠']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict('밥 먹어')#끝음에 민감함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
